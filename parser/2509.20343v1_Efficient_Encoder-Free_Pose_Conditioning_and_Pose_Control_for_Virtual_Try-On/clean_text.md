

# [PAGE 1]
Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual Try-On
Qi Li1*
Shuwen Qiu2 *†
Julien Han1
Xingzi Xu1,3 †
Mehmet Saygin Seyfioglu1
Kee Kiat Koo1
Karim Bouyarmane1
1Amazon
2University of California, Los Angeles (UCLA)
3Duke University
{qlimz,hameng,xingzixu,mseyfiog,kiatkoo,bouykari}@amazon.com
xingzi.xu@duke.edu
jantqiu@cs.ucla.edu
https://pose-vton.github.io/vto-pose-conditioning/
Work submitted in November 2024 to CVPR 2025
Abstract
As online shopping continues to grow, the demand for
Virtual Try-On (VTON) technology has surged, allowing
customers to visualize products on themselves by overlay-
ing product images onto their own photos. An essential
yet challenging condition for effective VTON is pose con-
trol, which ensures accurate alignment of products with
the user’s body while supporting diverse orientations for
a more immersive experience. However, incorporating pose
conditions into VTON models presents several challenges,
including selecting the optimal pose representation, inte-
grating poses without additional parameters, and balanc-
ing pose preservation with flexible pose control.
In this work, we build upon a baseline VTON model
that concatenates the reference image condition without ex-
ternal encoder, control network, or complex attention lay-
ers. We investigate methods to incorporate pose control
into this pure-concatenation paradigm by spatially concate-
nating pose data, comparing performance using pose maps
and skeletons, without adding any additional parameters or
module to the baseline model. Our experiments reveal that
pose stitching with pose maps yields the best results, en-
hancing both pose preservation and output realism. Addi-
tionally, we introduce a mixed-mask training strategy using
fine-grained and bounding box masks, allowing the model
to support flexible product integration across varied poses
and conditions.
Our contributions are threefold: 1) We explore differ-
ent configurations for integrating pose representations into
VTON models, 2) We propose a lightweight, parameter-
efficient approach for adding pose control, and 3) We en-
*Equal contribution.
†Work done during internship at Amazon.
able flexible pose generation through mixed-mask training.
Evaluations on public benchmarks demonstrate that our
method improves pose preservation and outperforms state-
of-the-art models with more complex conditioning frame-
works, advancing VTON’s adaptability and realism for di-
verse real-world applications.
1. Introduction
As online shopping grows in popularity, so does the demand
for customers to “try on” products virtually by superimpos-
ing them onto their own photos. This process, known as Vir-
tual Try-On (VTON) [1–3], involves using a source image
provided by the user, a specified mask area (of the source
image), and an image of the desired product to try on (ref-
erence image). The model then seamlessly integrates the
product into the masked area of the user provided source im-
age, as shown in Fig. 1. Recent advances in UNet-based La-
tent Diffusion Models (LDMs) [4–7] have enabled VTON
to be formulated as an image-conditioned inpainting prob-
lem, which is fine-tuned with diffusion-based pre-trained
weights and has proven to achieve promising generation
quality.
A crucial aspect of VTON tasks is pose control. Pose in-
formation is essential for VTON for several reasons. First,
when the mask obscures parts of the user’s pose in the
source image, the model may “hallucinate” limbs or other
body parts, which is particularly challenging for generative
models to render realistically—especially with hands. This
often results in less satisfactory or realistic outputs, as seen
in Fig. 2. Additionally, incorporating pose control enables
VTON models to produce results with various user poses
and orientations, enhancing the diversity and realism of
arXiv:2509.20343v1  [cs.CV]  24 Sep 2025

[EQ eq_p0_000] -> see eq_p0_000.tex


# [PAGE 2]
Figure 1. Pose preservation and control. In the first row, pose preservation requires the model to generate natural try-on results while
maintaining the user’s original pose, even when parts of the body are obscured by a mask. In the second row, pose control tasks the model
with generating images of the user in different poses, guided by an input pose image. All examples shown are generated by our model.
user experiences. Although crucial, integrating poses into
VTON models presents several challenges: 1) Pose Rep-
resentation: Common pose representations include pose
maps (e.g. [8]) and skeletons (e.g.
[9]). Current VTON
approaches may use either representation or combine them,
but it remains unclear which works best for VTON tasks.
2) Integration with Model Inputs: Current models add
pose conditions as additional input channels, which requires
modifications to the original model weights and additional
training parameters. This raises the question of whether
pose conditions can be incorporated without extra param-
eters. 3) Pose Control vs. Pose Preservation: In addition
to maintaining the user’s original pose, VTON models must
also support customization by generating try-on results with
varying user poses. Unlike general image editing, VTON
models generate within a masked area, which can hinder
realistic outputs if the mask is not aligned with the pose.
In this work, we address these challenges by building
on efficient VTON models like DiT-VTON and DEFT-
VTON [1–3], a paradigm that simplified to the extreme the
VTON task by removing any need for external image en-
coders, additional encoding networks (”garment networks”,
”reference networks”, ”person network” etc), or complex
cross-attention layers. DiT-VTON simply concatenates the
masked source image and the reference image along the
spatial dimension and feeds the concatenated result into the
main diffusion network. This approach achieves simpler
and more efficient training while maintaining high-quality
generation and detail preservation, outperforming complex
architecture alternatives. Motivated by the performance, we
investigate whether additional conditions like poses can be
further concatenated in the spatial dimension without in-
troducing extra parameters. We use the pose conditioning
signal as the additional signal, due to the observed limita-
tion of baseline model in this domain (pose hallucination,
hand hallucination when hand is not visible in the reference,
[FIGURE img_p1_000]


# [PAGE 3]
Figure 2. A hard case for pose preservation: When body parts
fall within the masked area, the model must hallucinate the pose,
and cause natural hand appearance.
etc). We evaluate both pose concatenation and pose stitch-
ing techniques, as well as their performance with pose maps
and skeletons. Our results indicate that pose stitching with
pose maps produces the best outcomes.
Furthermore, we train our model using both fine-grained
masks and bounding box masks, enabling it to adapt flexibly
to diverse mask conditions. The model learns to fit products
precisely within fine-grained masks while allowing more
versatile generation with bounding box masks, supporting
varied pose-controlled outputs.
Our contributions unfold in three dimensions: 1) We
systematically study configurations for integrating differ-
ent pose representations into VTON models; 2) We pro-
pose a simple yet efficient method to incorporate pose con-
ditions without additional channels or extra model param-
eters; and 3) We enable flexible pose generation through
a mixed-mask training strategy. Evaluations on public test
benchmarks VITON-HD [10] and DressCode [11] demon-
strate that our method enhances pose preservation and sur-
passes state-of-the-art (SOTA) VTON models that rely on
more complex pose-conditioning frameworks.
2. Related Work
Image Virtual Try-On with Diffusion Models Latent Dif-
fusion Models (LDMs) [4–7] have demonstrated strong
generative performance across tasks such as text-to-image
generation, image inpainting, and image editing. To adapt
diffusion models for VTON tasks, WarpDiffusion [12] in-
tegrates a warping module [13–16] that aligns the gar-
ment with the masked area and uses a pre-trained text-to-
image diffusion model enhanced by a novel information
and local garment feature attention mechanism.
TryOn-
Diffusion [17] introduces a dual-UNet structure with cross-
attention to implicitly incorporate the warping process, link-
ing the garment condition with the model.
For better
garment representation infusion into the denoising UNet,
LaDI-VTON [18] applies a textual inversion module to em-
bed garment-specific information.
Moving beyond warping, StableVTON [19] incorporates
a supervision signal that aligns the attention map with the
warped clothing, while IDM-VTON [20] combines textual
inversion and a dual-UNet structure to achieve robust try-
on results with diverse, real-world images.
Further ex-
tensions in VTON, such as Anyfit [21], MV-VTON [22],
and Wear-Any-Way [23], introduce capabilities for multi-
garment, multi-view, and interactive editing. Meanwhile,
some works shift towards a single UNet structure. CatV-
TON [24] reduces complexity by removing extra garment
modules and cross-attention layers; TPD [25] uses a single
UNet with an additional mask channel to refine the mask in-
put; MMTryon [26] leverages additional encoders for multi-
modal, multi-reference generation, while M&M VTO [27]
incorporates a Diffusion Transformer and supplementary
encoders to enable style-controlled generation.
Adding Control to Diffusion Models Diffusion models
for text based image editing have shown superior perfor-
mances. However, abstract text prompting cannot provide
details about visual concepts such as poses, gestures, tex-
tures, etc. A conventional way to introduce control signals
into the diffusion models involves using a copy of the UNet
encoder as the ControlNet [28], which encodes the control
images such as sketches, poses, and depth information into
the denosing UNet. Text inversion with IP Adapter [29] and
DreamBooth [30] offers another alternative for additional
conditioning. In VTON task specifically, pose signal is an
important factor to help preserve and generate various poses
for the given subject.
Most works add additional chan-
nels to accommodate the pose images [18–20, 22, 25, 31].
Meanwhile, two types of pose representations — pose maps
(e.g. [8]) and skeletons (e.g. [9]) — are mixedly used with-
out validations. In this work, we take a closer look at the
pose conditioning and study the performance of different
pose representations and the effective and efficient way to
incorporate pose signals into the generation process.
3. Method
3.1. Preliminary
3.1.1. Latent Diffusion Models
Latent Diffusion Models [4, 32] operate by learning a grad-
ual denoising process through a series of timesteps that re-
verse a noise-injection process. Given an image x0, the for-
ward diffusion process adds Gaussian noise step-by-step,
creating a latent variable xt at each timestep t. This process
[FIGURE img_p2_001]

[EQ eq_p2_000] -> see eq_p2_000.tex


# [PAGE 4]
Figure 3. Adding pose conditions. The top image illustrates the construction of the input using pose stitching, while the bottom image
shows the model input with pose concatenation.
is represented as:
xt = √αt x0 +
√
1 −αt ϵ,
where αt is a noise schedule parameter and ϵ ∼N(0, I)
is Gaussian noise. The model is then trained to predict the
noise ϵ and gradually denoise xt back to x0 over multiple
steps. Stable Diffusion optimizes this process by working
within a compressed latent space, enabling high-resolution
generation with reduced computational requirements.
3.1.2. Denoising Diffusion Implicit Models (DDIM)
DDIM [33] builds on diffusion models by introducing a de-
terministic sampling method that accelerates the reverse dif-
fusion process. Unlike traditional diffusion, which follows
a Markovian sequence, DDIM allows for non-Markovian
sampling with fewer steps. The reverse process in DDIM
can be written as:
xt−1 = √αt−1 ˆx0 +
p
1 −αt−1 ϵt,
where ˆx0 is an estimate of the original image at each
timestep. This approach enables DDIM to achieve high-
quality results with significantly fewer sampling steps, en-
hancing efficiency while maintaining image fidelity.
3.2. Adding Pose Conditions into the Diffusion
Models
Virtual try-on generates a composite image of a person
wearing a specified garment. The input includes a person
image Ip ∈R3×H×W , a garment image Ig ∈R3×H×W ,
and a binary mask M defining the editable area. The in-
painted person image, which serves as input, is denoted as
Im = Ip ⊗M. To enable precise pose control, a pose im-
age Ic is additionally extracted from Ip. These inputs—Im,
Ig, and Ic—are encoded into the latent space by a VAE en-
coder E: xm = E(Im), xg = E(Ig), xc = E(Ic) where
x ∈R4×H/8×W/8. The mask M is interpolated to the size
m ∈RH/8×W/8 to align with the latent representations.
3.2.1. Stitching Conditions in the Spatial Dimension
As shown in recent work [1, 20, 24, 25, 34], stitching
a reference image with the masked image along the spa-
tial dimension can effectively preserve text and logo de-
tails in garments. Specifically, Stable Diffusion (SD) in-
painting models extend the original 4-channel noisy im-
age into 9 channels by concatenating zt, xm, and m in
the channel dimension, denoted as concatc. To integrate
an additional garment image xg, it is concatenated along-
side the masked image in the spatial dimension, denoted
as concats. Additional channels are padded with black im-
ages xα ∈R4×H/8×W/8 and xβ ∈RH/8×W/8: xmasked =
concats([xm, xg]), xmask
=
concats([m, xβ]), xnoisy
=
concats([xt, xα]).
The final input to the transformer blocks is xin
=
concatc[xnoisy, xmasked, xmask]. The model’s output is then
cut in half.
3.3. Adding Pose Constraints
Above, we considered the minimal conditions for the
VTON task. Additional conditions, such as pose and depth
information, can help better preserve the pose within the
[FIGURE img_p3_002]

[EQ eq_p3_000] -> see eq_p3_000.tex

[EQ eq_p3_001] -> see eq_p3_001.tex

[EQ eq_p3_002] -> see eq_p3_002.tex

[EQ eq_p3_003] -> see eq_p3_003.tex

[EQ eq_p3_004] -> see eq_p3_004.tex

[EQ eq_p3_005] -> see eq_p3_005.tex

[EQ eq_p3_006] -> see eq_p3_006.tex


# [PAGE 5]
Table 1. Quantitative comparison on VTION-HD dataset of different methods for combining pose representation (joints vs. pose maps),
integration methods (stitching vs. concatenation), and color modes (color vs. grayscale).
Pose conditions
Stable Diffusion v1.5
Stable Diffusion XL
SSIM↑
LPIPS↓
FID↓
KID↓
SSIM↑
LPIPS↓
FID↓
KID↓
Ours w/ Joints Stitch
0.8861
0.0958
9.052
1.175
0.8864
0.0941
10.091
2.088
Ours w/ Joints Concat
0.8781
0.1087
10.467
1.725
0.8695
0.1078
12.517
3.734
Ours w/ Pose Concat
0.8870
0.1148
12.623
3.277
0.8861
0.0928
11.895
3.144
Ours w/ Pose Concat Gray
0.8879
0.1120
12.449
3.256
0.8856
0.0918
11.809
2.975
Ours w/ Pose Stitch Gray
0.9053
0.0694
8.646
0.872
0.8993
0.0766
9.341
1.367
Table 2. Quantitative comparison with baselines on the VITON-HD and DressCode test sets. Models labeled with /w SD and SDXL use
Stable Diffusion v1.5 and Stable Diffusion XL as the backbone, respectively.
Models
VITONHD
DressCode
SSIM↑
LPIPS ↓
FID↓
KID↓
SSIM↑
LPIPS ↓
FID ↓
KID ↓
StableVTON [19]
0.8543
0.0905
11.054
3.914
-
-
-
-
LaDI-VTON [18]
0.8603
0.0733
14.648
8.754
0.7656
0.2366
10.676
5.787
IDM-VTON [20]
0.8499
0.0603
9.842
1.123
0.8797
0.0563
9.546
4.320
CatVTON [24]
0.8704
0.0565
9.015
1.091
0.8922
0.0455
6.137
1.403
Ours w/ SD
0.9053
0.0694
8.646
0.872
0.9277
0.0510
5.103
0.951
Ours w/ SDXL
0.8993
0.0766
9.341
1.367
0.9252
0.0532
5.545
1.358
masked area. In practice, there are two primary represen-
tations of pose: pose maps and skeletons. A pose map is a
dense representation, often visualized as a heatmap or color-
coded overlay, where each pixel intensity indicates the like-
lihood of a specific body part being located at that position
(see, e.g. [8]). Skeleton joints, on the other hand, provide a
sparse representation of body pose, typically using a set of
key points (or ”joints”) representing crucial body parts such
as elbows, knees, wrists, and shoulders. These joints are
connected by lines to form a skeleton-like structure, repre-
senting the person’s pose (see, e.g. [9]). For each type of
representation, we explore two possible methods in Fig. 3
without introducing additional training parameters to the
model, making them adaptable to any VTON model:
• Concatenate Pose in the Spatial Dimension.
A
straightforward approach to incorporating additional
conditions is to concatenate them after the reference
image:
xmasked
=
concats([xm, xg, xc]), xmask
=
concat([m, xβ, xβ]), xnoisy = concat([xt, xα, xα])
The final output only utilizes the first third along the spa-
tial dimension.
• Stitch Pose into the Masked Area. Since the pose con-
straint originates from the source image, we can inte-
grate the pose and masked image as a single image using
Im = Ip ⊗M + Ic ⊗(1 −M).
3.3.1. Masking Strategy
In inpainting tasks, the mask plays a crucial role in marking
the boundary of the context and conveying the user’s intent.
For various pose conditioning scenarios, we consider corre-
sponding masking strategies:
• Fine-grained Mask: In this scenario, users employ the
mask to precisely define the boundary of the inpainting
area, indicating how the clothing should be overlaid onto
the person. This strategy is primarily used for pose preser-
vation.
• Bounding Box Mask: When users aim to adjust the orig-
inal pose by providing a pose image not derived from the
source image, the fine-grained mask may not effectively
capture the intended boundary. To offer greater flexibility
in model generation for different poses, a bounding box is
obtained by find the minimal rectangular that covers the
fine-grained mask. It is used to represent the approximate
area being edited.
During training, the fine-grained and bounding box
masks are applied alternately to help the model recognize
and differentiate between these distinct intentions.

[EQ eq_p4_000] -> see eq_p4_000.tex

[EQ eq_p4_001] -> see eq_p4_001.tex

[EQ eq_p4_002] -> see eq_p4_002.tex

[EQ eq_p4_003] -> see eq_p4_003.tex


# [PAGE 6]
Figure 4. Illustration of different methods for combining pose representation (joints vs. pose map), integration techniques (stitching vs.
concatenation), and color modes (color vs. grayscale).
Figure 5. Qualitative comparison of baseline pose-free/pose-less models and our methods for pose preservation.
4. Experiment
4.1. Datasets
We conduct our tests on two publicly available virtual try-on
datasets: VITON-HD [10] and DressCode [11]. Each input
image is padded and resized to a resolution of 512×512.
VITON-HD contains 2,032 image pairs of upper-body gar-
ments in its test split. We utilize the preprocessed masks
provided in this dataset. DressCode comprises 5,400 testing
pairs across three garment categories (upper-body, lower-
body, and full dresses).
4.2. Pose Representations
For pose maps, VITON-HD test set provides preprocessed
pose maps directly within the dataset, while for DressCode,
the results extracted from a pose map representation model
(similar to
[8]).
We then apply the same color map as
VITON-HD to generate pose visualization images. For joint
skeletons, we generate joint points using a joints pose rep-
resentation model.
4.3. Adding Pose Conditions
To integrate pose conditions into the model inputs, we ex-
periment with two pose representations (skeleton joints and
pose maps) and two input formats (concatenation and stitch-
ing). Additionally, preliminary studies suggest that color in
pose images can interfere with generation quality. There-
fore, we test both color and grayscale formats for pose and
skeleton images, resulting in five control groups, as illus-
trated in Fig. 4:
• Joints Stitch: Stitching the skeleton joints image into the
masked regions.
• Joints Concat: Concatenating the skeleton joints image
along the spatial dimension, adjacent to the reference im-
age.
• Pose Concat: Concatenating the pose map image along
[FIGURE img_p5_003]
[FIGURE img_p5_004]


# [PAGE 7]
the spatial dimension, adjacent to the reference image.
• Pose Concat Gray:
Concatenating the grayscale-
converted pose map image along the spatial dimension,
adjacent to the reference image.
• Pose Stitch Gray: Stitching the grayscale-converted pose
map image into the masked regions.
4.4. Implementation Details
We employ Stable Diffusion v1.5 as our diffusion backbone
models [4, 32]. The model is trained with a learning rate of
1e-5 and a batch size of 8. We use DDIM with 25 sampling
steps and a guidance scale of 5. Both diffusion models are
fine-tuned for 20 epochs on 4 NVIDIA H100 80G GPUs.
During training, we use 50% fine-grained masks and 50%
bounding box masks.
4.5. Baselines and Evaluation Metrics
We select the latest baseline results from CatVTON [24],
StableVTON [19], LaDI-VTON [18], IDM-VTON [20]
To evaluate the model’s output, we employ the widely
used evaluation metrics:
Structural SIMilarity Index
(SSIM) [35] and Learned Perceptual Image Patch Similarity
(LPIPS) [36], comparing generated results to ground truth
images. In the unpaired setting, where the garment in the
person image differs from the input garment image, we uti-
lize Fr´echet Inception Distance (FID) [36] and Kernel In-
ception Distance (KID) [37] as additional metrics. Our im-
plementation follows that in [18].
5. Results
5.1. Quantitative Results
5.1.1. Pose Maps vs Skeleton Joints
Comparing pose concatenation rows with joints and pose
maps (see Tab. 1), we observe that pose maps consistently
outperform skeleton joints across all metrics. For stitched
results, the grayscale pose maps row yield superior out-
comes than the joints stitching row. This suggests that pose
maps capture richer pose information, which enhances the
quality of the generated results.
5.1.2. Concatenating vs Stitching
Within each pose representation, we compare the concate-
nation and stitching methods. As shown in Tab. 1, stitching
outperforms concatenation for both representations. Addi-
tionally, converting pose maps to grayscale does not en-
hance performance in concatenation methods but signifi-
cantly improves stitching methods.
This finding implies
that direct concatenation may not fully leverage the pose in-
formation. To better integrate the pose map, stitching with
grayscale pose maps is preferred.
5.1.3. SD vs SDXL
In Tab. 1, a comparison between the left and right sections
shows that SDXL and SD v1.5 achieve comparable results.
Observations regarding pose representations and integration
methods are consistent across both model architectures.
5.1.4. Comparison with Baseline Results
In Tab. 2, we explore the potential benefits of pose condi-
tioning on VTON performance. Building on the original
encoder-free VTON architectures in [1, 34] we stitch the
grayscale pose map into the masked area and test both SD
v1.5 and SDXL inpainting models as backbones. Integrat-
ing the pose image without introducing additional training
parameters consistently improves the original results and
outperforms other baselines that add an extra channel for
the pose image. This comparison demonstrates that stitch-
ing grayscale maps can enhance generation results in an ef-
fective manner.
5.2. Qualitative Results
5.2.1. Pose Preservation
Fig. 5 compares a pose-less baseline model and our model
in terms of pose preservation. When parts of the body are
masked (especially limb extremities like hands and feet),
pose-less model often misinterprets the pose, and in the sec-
ond example, even alters the T-shirt to a long-sleeve style.
In contrast, our model, guided by pose conditions, more ac-
curately preserves the original pose and retains the clothing
style.
5.2.2. Pose Control
In Fig. 6, we illustrate how our model adapts to different
poses based on the input pose control signals. We com-
pare our results with IDM-VITON ( we use a new pose im-
age as its input during inference to guide generation). Our
method effectively generates poses that differ from those
in the original source images, offering flexible user-defined
poses.
While IDM-VITON accurately reflects the target
pose, its outputs appear less natural in comparison.
5.2.3. Ablations
Fig. 7 presents the generation results from the five control
groups. We observe that pose concatenation introduces un-
natural color artifacts, while joint stitching produces reason-
able color but tends to overfit within the masked area. The
grayscale pose stitching method, in comparison, more accu-
rately captures both the pose and body shape of the person.
Additional comparisons of baseline pose-free/pose-less
models and our methods for pose preservation, as well as
failure cases of pose-free model are presented in Fig. 8,
5.2.4. Limitations
In Fig. 9, we illustrate a failure case where the input pose
image is misaligned with the context outside the bounding


# [PAGE 8]
Figure 6. Qualitative comparison of IDM-VTON and our methods for pose-controlled try-on generation.
Figure 7. Qualitative comparison of the five methods for pose integration into the model input.
box, causing the generated results to deviate from the ex-
pected pose. This highlights the importance of maintaining
consistent depth and positioning of the human figure rela-
tive to the original pose to achieve more natural results. Ex-
tending the model to handle more diverse poses with varia-
tions in position and depth will be explored in future work.
6. Conclusion
In this work, we explore different pose representations and
ways to condition pose as the VTON model input. Exper-
imental results show that stitching grayscale pose map into
the masked area of the source image achieves the best gen-
eration results and improve the original base model. Mean-
while, it outperforms other baselines that use additional
channel to accommodate pose signals. Our method can be
flexibly adapted to any VTON models. It is shown to not
only preserve the original pose but also generate versatile
poses of the original user.
[FIGURE img_p7_005]
[FIGURE img_p7_006]


# [PAGE 9]
Figure 8. Additional comparisons of baseline pose-free/pose-less models and our methods for pose preservation. We show typical failure
cases of pose-free model.
[FIGURE img_p8_007]


# [PAGE 10]
Figure 9. A failure case where the provided pose image is mis-
aligned with the original unmasked context in depth and position.
[FIGURE img_p9_008]


# [PAGE 11]
References
[1] Qi Li, Shuwen Qiu, Julien Han, Xingzi Xu, Mehmet Saygin
Seyfioglu, Kee Kiat Koo, and Karim Bouyarmane. Dit-vton:
Diffusion transformer framework for unified multi-category
virtual try-on and virtual try-all with integrated image edit-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) Workshops,
2025. AI for Content Creation Workshop. 1, 2, 4, 7
[2] Julien Han, Shuwen Qiu, Qi Li, Xingzi Xu, Mehmet Saygin
Seyfioglu, Kavosh Asadi, and Karim Bouyarmane. Instructv-
ton: Optimal auto-masking and natural-language-guided in-
teractive style control for inpainting-based virtual try-on. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) Workshops, 2025. AI
for Content Creation Workshop.
[3] Xingzi Xu, Qi Li, Shuwen Qiu, Julien Han, and Karim Bou-
yarmane. Deft-vton: Efficient virtual try-on with consistent
generalised h-transform. In Proceedings of the Computer Vi-
sion and Pattern Recognition Conference, pages 3308–3317,
2025. 1, 2
[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-resolution image
synthesis with latent diffusion models.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. 1, 3, 7
[5] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim
Entezari, Jonas M¨uller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling recti-
fied flow transformers for high-resolution image synthesis.
In Forty-first International Conference on Machine Learn-
ing, 2024.
[6] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021.
[7] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding.
Advances in neural information
processing systems, 35:36479–36494, 2022. 1, 3
[8] Rıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.
Densepose: Dense human pose estimation in the wild. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 7297–7306, 2018. 2, 3, 5, 6
[9] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.
Sheikh. Openpose: Realtime multi-person 2d pose estima-
tion using part affinity fields. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 2019. 2, 3, 5
[10] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul
Choo.
Viton-hd:
High-resolution virtual try-on via
misalignment-aware normalization.
In Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition, pages 14131–14140, 2021. 3, 6
[11] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico
Landi, Fabio Cesari, and Rita Cucchiara. Dress code: High-
resolution multi-category virtual try-on. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 2231–2235, 2022. 3, 6
[12] Xiu Li, Michael Kampffmeyer, Xin Dong, Zhenyu Xie,
Feida Zhu, Haoye Dong, Xiaodan Liang, et al.
Warpdif-
fusion: Efficient diffusion model for high-fidelity virtual try-
on. arXiv preprint arXiv:2312.03667, 2023. 3
[13] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry S
Davis. Viton: An image-based virtual try-on network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 7543–7552, 2018. 3
[14] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin
Chen, Liang Lin, and Meng Yang. Toward characteristic-
preserving image-based virtual try-on network. In Proceed-
ings of the European conference on computer vision (ECCV),
pages 589–604, 2018.
[15] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye
Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang.
Gp-
vton: Towards general purpose virtual try-on via collabora-
tive local-flow global-parsing learning.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 23550–23559, 2023.
[16] Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei
Liu, and Ping Luo. Parser-free virtual try-on via distilling
appearance flows.
In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
8485–8493, 2021. 3
[17] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William
Chan, Chitwan Saharia, Mohammad Norouzi, and Ira
Kemelmacher-Shlizerman.
Tryondiffusion: A tale of two
unets. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 4606–4615,
2023. 3
[18] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Mar-
cella Cornia, Marco Bertini, and Rita Cucchiara. Ladi-vton:
Latent diffusion textual-inversion enhanced virtual try-on. In
Proceedings of the 31st ACM International Conference on
Multimedia, pages 8580–8589, 2023. 3, 5, 7
[19] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park,
and Jaegul Choo. Stableviton: Learning semantic correspon-
dence with latent diffusion model for virtual try-on. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 8176–8185, 2024. 3, 5, 7
[20] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon
Choi, and Jinwoo Shin. Improving diffusion models for vir-
tual try-on. arXiv preprint arXiv:2403.05139, 2024. 3, 4, 5,
7
[21] Yuhan Li, Hao Zhou, Wenxiang Shang, Ran Lin, Xuanhong
Chen, and Bingbing Ni.
Anyfit: Controllable virtual try-
on for any combination of attire across any scenario. arXiv
preprint arXiv:2405.18172, 2024. 3
[22] Haoyu Wang, Zhilu Zhang, Donglin Di, Shiliang Zhang, and
Wangmeng Zuo. Mv-vton: Multi-view virtual try-on with
diffusion models. arXiv preprint arXiv:2404.17364, 2024. 3
[23] Mengting Chen, Xi Chen, Zhonghua Zhai, Chen Ju, Xuewen
Hong, Jinsong Lan, and Shuai Xiao. Wear-any-way: Manip-
ulable virtual try-on via sparse correspondence alignment.
arXiv preprint arXiv:2403.12965, 2024. 3

[EQ eq_p10_000] -> see eq_p10_000.tex

[EQ eq_p10_001] -> see eq_p10_001.tex


# [PAGE 12]
[24] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang,
Wenqing Zhang, Xujie Zhang, Hanqing Zhao, and Xiaodan
Liang. Catvton: Concatenation is all you need for virtual try-
on with diffusion models. arXiv preprint arXiv:2407.15886,
2024. 3, 4, 5, 7
[25] Xu Yang, Changxing Ding, Zhibin Hong, Junhao Huang,
Jin Tao, and Xiangmin Xu.
Texture-preserving diffusion
models for high-fidelity virtual try-on.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7017–7026, 2024. 3, 4
[26] Xujie Zhang, Ente Lin, Xiu Li, Yuxuan Luo, Michael
Kampffmeyer, Xin Dong, and Xiaodan Liang.
Mmtryon:
Multi-modal multi-reference control for high-quality fashion
generation. arXiv preprint arXiv:2405.00448, 2024. 3
[27] Luyang Zhu, Yingwei Li, Nan Liu, Hao Peng, Dawei
Yang, and Ira Kemelmacher-Shlizerman. M&m vto: Multi-
garment virtual try-on and editing.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1346–1356, 2024. 3
[28] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models.
In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 3836–3847, 2023. 3
[29] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-
adapter: Text compatible image prompt adapter for text-to-
image diffusion models. arXiv preprint arXiv:2308.06721,
2023. 3
[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation.
In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 22500–
22510, 2023. 3
[31] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shi-
long Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao.
Zero-shot image editing with reference imitation.
arXiv
preprint arXiv:2406.07547, 2024. 3
[32] Dustin
Podell,
Zion
English,
Kyle
Lacey,
Andreas
Blattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and
Robin Rombach.
Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis.
arXiv preprint
arXiv:2307.01952, 2023. 3, 7
[33] Jiaming
Song,
Chenlin
Meng,
and
Stefano
Ermon.
Denoising diffusion implicit models.
arXiv preprint
arXiv:2010.02502, 2020. 4
[34] Xingzi Xu, Qi Li, Shuwen Qiu, Julien Han, and Karim Bou-
yarmane. Deft-vton: Efficient virtual try-on with consistent
generalised h-transform. https://deft-vton.github.io/, 2024. 4,
7
[35] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004. 7
[36] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586–595, 2018. 7
[37] Mikołaj Bi´nkowski, Danica J Sutherland, Michael Arbel, and
Arthur Gretton. Demystifying mmd gans. arXiv preprint
arXiv:1801.01401, 2018. 7
